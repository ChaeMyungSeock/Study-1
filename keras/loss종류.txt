[평가 지표]

< 1.회귀 평가 >
: 우리가 수치를 넣었을 때 수치로 답을 해줌.

1. MSE  (Mean Squared Error,평균 제곱 오차)         : sigma(실제값 - 예측값)^2 /n
2. RMSE (Root Mean Squared Error, 평균 제곱근 오차) : root[sigma(실제값 - 예측값)^2 /n]
                                                     MSE에 비해 에러값의 크기가 실제 값에 비례한다.(제곱된 것에 루트를 씌었기 때문)
   [RMSE함수]
   from sklearn.metrics import mean_squared_error
   # tensorflow, keras가 나오기 전에 쓰던 API

   def RMSE(y_test, y_predict):
       return np.sqrt(mean_squared_error(y_test, y_predict))
               # sprt() : root를 씌어줌

   print("RMSE : ", RMSE(y_test, y_predict))
3. R2     (R Squared, 설명력, 결정계수)             : (q1 - q2)/q1
                                                     q1 = (전체 데이터들의 편차의 제곱) 
                                                     q2 = (전체 데이터들의 잔차의 제곱)
                                                     1 에 근접할 수록 정확도가 좋다.
                                                    보통 다른 보조 지표와 함께 사용된다.

< 2.분류 평가 >
: 결과값에 대한 분류가 정해져 있어야 함. ex) 0과 1, 강아지와 고양이

1. (Precision, 정밀도) : TP / (TP + FP)
                       : 모델이 True라고 분류한 것 중에서 실제 True인 것의 비율
2. (Recall, 재현율)    : TP / (TP + FN)
                       : 실제 True인 것에서 모델이 True라고 예측한 것의 비율
3. acc (Accuracy, 정화도) : (TP + TN) / (TP + FN + FP + TN)
                          : True가 True로 나오는 것과 Flase가 Flase로 나오는 것도 정확하다고 본다.
                          : 만약 Flase 값이 많은 데이터라면 Flase를 유추하는 것에서는 정확도가 높지만
                            True를 유추하는 것에는 상대적으로 취약할 수 있음 (bias때문)
4. F1 score (Precision과 Recall의 조화평균) : 2*(Precision * Recall) / (Precision + Recall)
                                           : bias가 작다
5
