------------------------------------------------------------------------

#1. MinMax_Scaler   ( 정규화 )  =  ( x - min ) / ( max - min )
  : 데이터를 0 ~ 1 사이의 값으로 변환시킴 

------------------------------------------------------------------------

#2. Standard_Scaler ( 표준화 )  =  ( x - x평균) / x표준편차      
  : 0을 기준으로 모임 
  : data 최대, 최소값을 모를 경우 사용
  : 각 피처의 평균을 0, 분산을 1로 변경 ( 모든 특성들이 같은 스케일 값을 사용)

   # 표준편차 = [ sigma ( x - x평균)^2 ] / n   

------------------------------------------------------------------------

#3. Robust_scaler               = ( x - Q1(X) ) / ( Q3(X) - Q2(X) )
  : Standard_Scaler에 의한 표준화 보다 동일한 값을 더 넓게 분포
  : 이상치를 포함하는 데이터를 표준화하는 경우

  # Q1(x) : x의 1분위수     ex) x = 100개 일 경우 : 25개씩 나눈 것중에 첫번째 묶음(25개)

------------------------------------------------------------------------

#4. MaxAbs_Scaler
  : 0을 기준으로 절대값이 가장 큰 수 가 1 or -1가 되도록 변환
  : MinMax_Scaler와 유사하나 음수와 양수값에 따른 분포 유지

------------------------------------------------------------------------
<<< 사용 방법 >>>
from sklearn.prreprocessing import MaxMinScaler, StandardScaler    -> 사용할 전처리 방식을 가져온다.
# scaler = MaxMinScaler()         -> MaxMinScaler를 slcaer라고 하겠다.
scaler = StandardScaler()         -> StandardScaler   ,,,
scaler.fit(x)                     -> x의 범위로 StandardScaler를 실행하고 그 값을 scaler에 저장한다.
x = scaler.transform(x)           -> scaler를 가져와서 x를 그 모양으로 변환시키겠다

##### 만약 다른 값도 x의 범위 내로 변화시킬려면 x와 모양이 같아야 한다.



# PCA ( Principal Component Analysis )
: 열(feature)이 너무 많은 데이터는 데이터의 차원이 크기 때문에 학습 속도가 느릴 뿐만아니라 
: 성능 또한 좋지 않을 가능성이 크다.

: coulumn들의 연관성을 구해 새로운 feature를 만든다. => 차원축소
: n_compenent  = : coulumn의 수

####쓰는 법
from keras.deposition import PCA
pca = PCA(n_components  = '열의 개수')
pca.fit('적용시킬 데이터')
x = pca.fit('적용시킬 데이터')


# feature_important
: 여러개의 feature중 가장 중요하다고 생각되는 feature을 골라서 사용하고 나머지는 버린다.
: model마다 선택하는 feature가 달라진다.

